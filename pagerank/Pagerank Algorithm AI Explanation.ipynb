{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cceab516",
   "metadata": {},
   "source": [
    "# Pagerank Algorithm: Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c719f",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f4b84",
   "metadata": {},
   "source": [
    "The Pagerank algorithm is developed by the co-founders of Google, Larry Page and Sergey Brin, as a research project on an optimal search engine algorithm at Stanford University. Sergey Brin conceptualized this algorithm using the `heuristic` of link popularity - the idea that the popularity of a webpage is directly correlated to the number of websites which link to it. Hector Garcia-Molina, Sergey's university advisor, as well as Scott Hassan and Alan Steremberg, were among the people critical in the development of this algorithm.\n",
    "\n",
    "The goal of this project is to construct an algorithm that is capable of calculating the PageRank values of each webpage, given a universe of websites, `corpus`. To achieve this goal, `Markov Chains` will be utilized. Refer to the section on `Markov Chains` for more details.\n",
    "\n",
    "Although this project aims to model the PageRank algorithm created by Larry Page and Sergey Brin, the knowledge of Markov Chains has vast applications in a multitude of domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d948cf",
   "metadata": {},
   "source": [
    "## Initialization & Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9b8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "DAMPING = 0.85\n",
    "SAMPLES = 10000\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 2:\n",
    "       sys.exit(\"Usage: python pagerank.py corpus\")\n",
    "    corpus = crawl(\"corpus0\")\n",
    "    print(corpus)\n",
    "    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)\n",
    "    print(f\"PageRank Results from Sampling (n = {SAMPLES})\")\n",
    "    for page in sorted(ranks):\n",
    "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
    "    ranks = iterate_pagerank(corpus, DAMPING)\n",
    "    print(f\"PageRank Results from Iteration\")\n",
    "    for page in sorted(ranks):\n",
    "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
    "\n",
    "\n",
    "def crawl(directory):\n",
    "    \"\"\"\n",
    "    Parse a directory of HTML pages and check for links to other pages.\n",
    "    Return a dictionary where each key is a page, and values are\n",
    "    a list of all other pages in the corpus that are linked to by the page.\n",
    "    \"\"\"\n",
    "    pages = dict()\n",
    "\n",
    "    # Extract all links from HTML files\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".html\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            contents = f.read()\n",
    "            links = re.findall(r\"<a\\s+(?:[^>]*?)href=\\\"([^\\\"]*)\\\"\", contents)\n",
    "            pages[filename] = set(links) - {filename}\n",
    "\n",
    "    # Only include links to other pages in the corpus\n",
    "    for filename in pages:\n",
    "        pages[filename] = set(\n",
    "            link for link in pages[filename]\n",
    "            if link in pages\n",
    "        )\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61735b62",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002d1ea",
   "metadata": {},
   "source": [
    "This project utilizes the concept of Markov Chains to estimate PageRank values. A Markov Chain is a mathematical model that experiences transitions from one state to another according to probability values. It is a `stochastic process` that fulfills the `Markov Property` - the probability of getting to a future state is only dependent on a `FINITE` number of past states.\n",
    "\n",
    "This problem will be modelled as a time-homogeneous Markov Chain. The probability of getting to a future state - the next website - is only dependent on the model's current state - the current website the hypothetical surfer is on. This makes the process completely `memoryless` - knowledge of past websites that a surfer traversed has zero influence in predicting the next websites they would visit.\n",
    "\n",
    "A Markov Chain is probabilistic, differing from other search problems like Tic-Tac-Toe and Nim demonstrated in this portfolio, which are deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d336c",
   "metadata": {},
   "source": [
    "## Modelling PageRank as a Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba99acc",
   "metadata": {},
   "source": [
    "To model this problem as a Markov Chain, the state space of this problem will be defined by the variable `corpus`, which contains a universe of webpages.\n",
    "\n",
    "For simplicity, consider a hypothetical universe, corpus, with webpages `a.html` to `z.html`. The function `transition_model` generates a `conditional probability distribution` of clicking on another page, given the current page the surfer is on.\n",
    "\n",
    "This function considers two scenarios:\n",
    "1. Suppose the hypothetical random surfer is at webpage `x.html`. `x.html` contains zero outgoing links - represented by `len(corpus[page] == 0)`. We then assume that he would click on any of the 26 webpages with equal probability. The probability of accessing each webpage would therefore be 1/26, or generalized to `1/len(corpus)`.\n",
    "\n",
    "2. Suppose the hypothetical random surfer is at webpage `y.html`, and `y.html` contains links to `x.html` and `z.html`.\n",
    "- With `1-damping_factor`, the random surfer would click on any of the 26 webpages in the hypothetical universe. Initialize the pagerank value of each page in the universe with this value.\n",
    "- Update the pagerank value for `x.html` and `z.html` by the formula (`damping_factor/ len(corpus[page]`). `Len(corpus[page])` represents the number of links on the page - in this case, `y.html` has 2 links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c22eb",
   "metadata": {},
   "source": [
    "## Damping Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f045c50",
   "metadata": {},
   "source": [
    "The damping factor is predetermined to be 0.85. A full discussion of the damping factor is out of the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_model(corpus, page, damping_factor):\n",
    "    \"\"\"\n",
    "    Returns transition model.\n",
    "    \"\"\"\n",
    "    tm = {}\n",
    "    \n",
    "    if len(corpus[page]) == 0:\n",
    "        for c in corpus:       \n",
    "            tm[c] = 1/ len(corpus)\n",
    "    \n",
    "    else:\n",
    "        for c in corpus:\n",
    "            tm[c] = (1-damping_factor)/len(corpus)\n",
    "            \n",
    "        for webpage in corpus[page]:\n",
    "            tm[webpage] += damping_factor / len(corpus[page])\n",
    "    \n",
    "    return tm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638d455",
   "metadata": {},
   "source": [
    "## Calculation of Pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e42fa7",
   "metadata": {},
   "source": [
    "Two different algorithms are used to elucidate the final pagerank values of a webpage, `sample_pagerank` and `iterate_pagerank`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5a59b",
   "metadata": {},
   "source": [
    "### Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217fb7c",
   "metadata": {},
   "source": [
    "Much like randomly rolling a dice 10000 times and recording its value, the sampling algorithm simulates the process of randomly surfing the internet and counts the number of times each webpage is clicked on, dividing each of the final count by n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c663dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pagerank(corpus, damping_factor, n):\n",
    "    \"\"\"\n",
    "    Returns pagerank values via sampling\n",
    "    \"\"\"\n",
    "    rank = {}\n",
    "    count = 0\n",
    "    \n",
    "    #initialize\n",
    "    for c in corpus:\n",
    "        rank[c] = 0.0\n",
    "    \n",
    "    while count < n:\n",
    "        \n",
    "        #randomly generate 1st page\n",
    "        if count == 0:\n",
    "            pg = np.random.choice(list(corpus.keys()))\n",
    "            \n",
    "            #generate transition model for page\n",
    "            transition = transition_model(corpus, pg, DAMPING)\n",
    "            \n",
    "            #increment p & count\n",
    "            rank[pg] += 1\n",
    "            count += 1\n",
    "            \n",
    "        else:\n",
    "            next_page = random.choices(list(transition.keys()), weights = list(transition.values()))            \n",
    "            transition = transition_model(corpus, next_page[0], DAMPING)\n",
    "            \n",
    "            rank[next_page[0]] += 1\n",
    "            count += 1\n",
    "            \n",
    "    for item in rank:\n",
    "        rank[item] = rank[item]/ n\n",
    "        \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b8c51",
   "metadata": {},
   "source": [
    "### Iterative Algorithm & its Convergence Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd03be",
   "metadata": {},
   "source": [
    "This algorithm utilizes the Power Iteration strategy to compute PageRank values. At t = 0, the initial probability distribution is assumed as 1/n, where n refers to the number of pages in the universe.\n",
    "\n",
    "At each time step, the algorithm updates the pagerank values of each page in the corpus using the following formula.\n",
    "\n",
    "This step continues until a convergence criteria is reached. According to Haveliwala et al of Stanford University, the greater the eigengap of the PageRank adjacency matrix, the more resilient it is to pertubations in the Markov chain. This causes the algorithm to rapidly converge after a few iterations, which may allow the algorithm to be generalized to a larger universe and deployed efficiently at high speeds.\n",
    "\n",
    "For a full discussion of this phenomenon, please refer to http://www-cs-students.stanford.edu/~taherh/papers/secondeigenvalue.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_pagerank(corpus, damping_factor):\n",
    "    \"\"\"\n",
    "    Returns pagerank values via iteration and updates values until convergence\n",
    "    \"\"\"\n",
    "    \n",
    "    ranked = {}\n",
    "    previous = {}\n",
    "    \n",
    "    #initialize\n",
    "    for link in corpus:\n",
    "        ranked[link] = 1/ len(corpus)\n",
    "    \n",
    "    converged = False\n",
    "    while not converged:\n",
    "            \n",
    "        for link in corpus:  \n",
    "                     \n",
    "            previous = ranked.copy()\n",
    "            \n",
    "            #update based on damping factor formula\n",
    "            for k, v in corpus.items():\n",
    "                if link in v and k != link:\n",
    "                    ranked[link] += damping_factor * (ranked[k]/len(v))\n",
    "            \n",
    "        #convergence test\n",
    "        for key in ranked:\n",
    "            if np.abs(ranked[key] - previous[key]) != 0.001:\n",
    "                converged = False\n",
    "        \n",
    "        converged = True    \n",
    "    \n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1d40e",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28deeba",
   "metadata": {},
   "source": [
    "By using sampling and Markov chain algorithms to model AI decision making under uncertainty (ie. in a random state instead of a deterministic state), one is able to develop similar algorithms to model problems with time-series properties. This applies to the field of quantitative finance, investments, economics and game theory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
