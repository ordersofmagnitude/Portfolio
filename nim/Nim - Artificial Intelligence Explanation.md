{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d826f56e",
   "metadata": {},
   "source": [
    "# Nim AI Explanation: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56749e63",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cfe1b",
   "metadata": {},
   "source": [
    "Nim is a two-player combinatorial game. Each player takes turns removing a minimum of one object, but all of the objects they remove have to be from the same pile.\n",
    "\n",
    "Nim is one of the most famous examples of an impartial game - a game where both players have the same moves all the time, and the 1st mover of the game always wins. It is also completely solved, in the sense that the exact strategy has been found for every starting configuration.\n",
    "\n",
    "The winner is the player who makes the last move.\n",
    "\n",
    "In this project, I use Q-learning, a type of reinforcement learning strategy, to train the AI 10000 times in order to play against a human player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bfab1",
   "metadata": {},
   "source": [
    "## AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05983b6f",
   "metadata": {},
   "source": [
    "### Setting up the configuration of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Nim():\n",
    "\n",
    "    def __init__(self, initial=[1, 3, 5, 7]):\n",
    "        \"\"\"\n",
    "        Initialize game board.\n",
    "        Each game board has\n",
    "            - `piles`: a list of how many elements remain in each pile\n",
    "            - `player`: 0 or 1 to indicate which player's turn\n",
    "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
    "        \"\"\"\n",
    "        self.piles = initial.copy()\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "\n",
    "    @classmethod\n",
    "    def available_actions(cls, piles):\n",
    "        \"\"\"\n",
    "        Nim.available_actions(piles) takes a `piles` list as input\n",
    "        and returns all of the available actions `(i, j)` in that state.\n",
    "\n",
    "        Action `(i, j)` represents the action of removing `j` items\n",
    "        from pile `i` (where piles are 0-indexed).\n",
    "        \"\"\"\n",
    "        actions = set()\n",
    "        for i, pile in enumerate(piles):\n",
    "            for j in range(1, pile + 1):\n",
    "                actions.add((i, j))\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        \"\"\"\n",
    "        Nim.other_player(player) returns the player that is not\n",
    "        `player`. Assumes `player` is either 0 or 1.\n",
    "        \"\"\"\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"\n",
    "        Switch the current player to the other player.\n",
    "        \"\"\"\n",
    "        self.player = Nim.other_player(self.player)\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Make the move `action` for the current player.\n",
    "        `action` must be a tuple `(i, j)`.\n",
    "        \"\"\"\n",
    "        pile, count = action\n",
    "\n",
    "        # Check for errors\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Game already won\")\n",
    "        elif pile < 0 or pile >= len(self.piles):\n",
    "            raise Exception(\"Invalid pile\")\n",
    "        elif count < 1 or count > self.piles[pile]:\n",
    "            raise Exception(\"Invalid number of objects\")\n",
    "\n",
    "        # Update pile\n",
    "        self.piles[pile] -= count\n",
    "        self.switch_player()\n",
    "\n",
    "        # Check for a winner\n",
    "        if all(pile == 0 for pile in self.piles):\n",
    "            self.winner = self.player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713f5c7",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22b8f8",
   "metadata": {},
   "source": [
    "The main focus on this project is the reinforcement learning algorithm. Reinforcement learning works by giving the algorithm a reward when it chooses an optimal state, and \"punishing\" the algorithm if it chooses a state that is deemed incorrect. In general, a reinforcement learning algorithm is able to perceive and interpret its environment and learn the optimal moves through trial and error.\n",
    "\n",
    "The reinforcement learning algorithm used here is Q-learning. Q-learning trains the algorithm to search for the combination of game moves that optimizes the q-value, which is the reward in this context. This concept is influenced by operant conditioning in psychology.\n",
    "\n",
    "Expounding upon the algorithm:\n",
    "\n",
    "- The function get_q_value obtains a q-value stored in a dictionary, self.q.\n",
    "\n",
    "- The function update_q_value updates the q-value based on the q-learning formula:\n",
    "Q(s, a) <- old value estimate\n",
    "                   + alpha * (new value estimate - old value estimate)\n",
    "\n",
    "- The function best_future_reward searches through all available actions in the Nim game and returns the q-value of the best move.\n",
    "\n",
    "- The function choose_action searches through all available actions:\n",
    "1. If there are no available actions, it returns None.\n",
    "2. The algorithm then searches for optimal actions with q_value equating to the best future reward.\n",
    "3. If the algorithm is greedy (epsilon = False), it aims to maximize the current best future reward, so the optimal action is returned.\n",
    "4. If the algorithm is not greedy (explorative), it returns the optimal action vs the random action based on a weighted probability, self.epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1162f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NimAI():\n",
    "\n",
    "    def __init__(self, alpha=0.5, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize AI with an empty Q-learning dictionary,\n",
    "        an alpha (learning) rate, and an epsilon rate.\n",
    "\n",
    "        The Q-learning dictionary maps `(state, action)`\n",
    "        pairs to a Q-value (a number).\n",
    "         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n",
    "         - `action` is a tuple `(i, j)` for an action\n",
    "        \"\"\"\n",
    "        self.q = dict()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, old_state, action, new_state, reward):\n",
    "        \"\"\"\n",
    "        Update Q-learning model, given an old state, an action taken\n",
    "        in that state, a new resulting state, and the reward received\n",
    "        from taking that action.\n",
    "        \"\"\"\n",
    "        old = self.get_q_value(old_state, action)\n",
    "        best_future = self.best_future_reward(new_state)\n",
    "        self.update_q_value(old_state, action, old, reward, best_future)\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the Q-value for the state `state` and the action `action`.\n",
    "        If no Q-value exists yet in `self.q`, return 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        #state: list, eg. [1, 1, 3, 5] means that \n",
    "        #pile 0 has 1 object, pile 1 has 1 object,\n",
    "        #pile 2 has 3 objects and pile 3 has 5 objects\n",
    "        \n",
    "        #action: i, j, taking j objects from pile i\n",
    "        \n",
    "        if (tuple(state), action) in self.q.keys():\n",
    "            return self.q[tuple(state), action]\n",
    "        return 0\n",
    "\n",
    "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
    "        \n",
    "        self.q[tuple(state), action] = old_q + self.alpha * (reward + future_rewards - old_q)\n",
    "\n",
    "    def best_future_reward(self, state):\n",
    "            \n",
    "        possible_actions = Nim.available_actions(state)\n",
    "        \n",
    "        #return 0 if no available actions\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0\n",
    "        \n",
    "        max_q = -np.inf\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            q_value = self.get_q_value(state, action)\n",
    "                \n",
    "            if q_value > max_q:\n",
    "                max_q = q_value\n",
    "            \n",
    "        return max_q\n",
    "\n",
    "    def choose_action(self, state, epsilon=True):\n",
    "    \n",
    "        #obtain all possible actions in the state        \n",
    "        avail = Nim.available_actions(state)\n",
    "        \n",
    "        if len(avail) == 0:\n",
    "            print(\"No available actions in this state\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        #list of optimal actions\n",
    "        optimal_actions = []\n",
    "        \n",
    "        for a in avail:\n",
    "            if self.get_q_value(state, a) == self.best_future_reward(state):\n",
    "                optimal_actions.append(a)\n",
    "                \n",
    "        print(\"list of optimal actions\", optimal_actions)\n",
    "        \n",
    "        #if greedy algorithm and optimal actions are present:\n",
    "        if not epsilon and len(optimal_actions) >= 1:\n",
    "            act = random.choice(optimal_actions)\n",
    "            return act\n",
    "        \n",
    "        else:\n",
    "            random_action = random.choice(list(avail))\n",
    "            optimal_action = random.choice(optimal_actions)\n",
    "            act = random.choices([optimal_action, random_action], weights = \\\n",
    "                                  [1- self.epsilon, self.epsilon], k = 1)\n",
    "            return act[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee2156",
   "metadata": {},
   "source": [
    "### Training and Playing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7890db",
   "metadata": {},
   "source": [
    "After training the AI for 10000 games, the human should never be able to win against the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7df619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n):\n",
    "    \"\"\"\n",
    "    Train an AI by playing `n` games against itself.\n",
    "    \"\"\"\n",
    "\n",
    "    player = NimAI()\n",
    "\n",
    "    # Play n games\n",
    "    for i in range(n):\n",
    "        print(f\"Playing training game {i + 1}\")\n",
    "        game = Nim()\n",
    "\n",
    "        # Keep track of last move made by either player\n",
    "        last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None}\n",
    "        }\n",
    "\n",
    "        # Game loop\n",
    "        while True:\n",
    "\n",
    "            # Keep track of current state and action\n",
    "            state = game.piles.copy()\n",
    "            action = player.choose_action(game.piles)\n",
    "            print(\"action chosen by AI...\", action)\n",
    "\n",
    "            # Keep track of last state and action\n",
    "            last[game.player][\"state\"] = state\n",
    "            last[game.player][\"action\"] = action\n",
    "\n",
    "            # Make move\n",
    "            game.move(action)\n",
    "            new_state = game.piles.copy()\n",
    "\n",
    "            # When game is over, update Q values with rewards\n",
    "            if game.winner is not None:\n",
    "                player.update(state, action, new_state, -1)\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    1\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # If game is continuing, no rewards yet\n",
    "            elif last[game.player][\"state\"] is not None:\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    print(\"Done training\")\n",
    "\n",
    "    # Return the trained AI\n",
    "    return player\n",
    "\n",
    "\n",
    "def play(ai, human_player=None):\n",
    "    \"\"\"\n",
    "    Play human game against the AI.\n",
    "    `human_player` can be set to 0 or 1 to specify whether\n",
    "    human player moves first or second.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no player order set, choose human's order randomly\n",
    "    if human_player is None:\n",
    "        human_player = random.randint(0, 1)\n",
    "\n",
    "    # Create new game\n",
    "    game = Nim()\n",
    "\n",
    "    # Game loop\n",
    "    while True:\n",
    "\n",
    "        # Print contents of piles\n",
    "        print()\n",
    "        print(\"Piles:\")\n",
    "        for i, pile in enumerate(game.piles):\n",
    "            print(f\"Pile {i}: {pile}\")\n",
    "        print()\n",
    "\n",
    "        # Compute available actions\n",
    "        available_actions = Nim.available_actions(game.piles)\n",
    "        print(\"available actions...\", available_actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Let human make a move\n",
    "        if game.player == human_player:\n",
    "            print(\"Your Turn\")\n",
    "            while True:\n",
    "                pile = int(input(\"Choose Pile: \"))\n",
    "                count = int(input(\"Choose Count: \"))\n",
    "                if (pile, count) in available_actions:\n",
    "                    break\n",
    "                print(\"Invalid move, try again.\")\n",
    "\n",
    "        # Have AI make a move\n",
    "        else:\n",
    "            print(\"AI's Turn\")\n",
    "            \n",
    "            #check what output does ai.choose_action give\n",
    "            #ensure that it can be elicited in a tuple form\n",
    "            \n",
    "            pile, count = ai.choose_action(game.piles, epsilon=False)\n",
    "            print(f\"AI chose to take {count} from pile {pile}.\")\n",
    "\n",
    "        # Make move\n",
    "        game.move((pile, count))\n",
    "\n",
    "        # Check for winner\n",
    "        if game.winner is not None:\n",
    "            print()\n",
    "            print(\"GAME OVER\")\n",
    "            winner = \"Human\" if game.winner == human_player else \"AI\"\n",
    "            print(f\"Winner is {winner}\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74d3ca",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3ee28",
   "metadata": {},
   "source": [
    "This is my 1st attempt at writing a reinforcement learning algorithm. Further developments of this algorithm can be used for other games, such as Knights and Chess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
