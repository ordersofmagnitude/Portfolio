{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d826f56e",
   "metadata": {},
   "source": [
    "# Nim AI Explanation: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56749e63",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cfe1b",
   "metadata": {},
   "source": [
    "Nim can be classified as a combinatorial adversarial game. A combinatorial game is a game that satisfies the following constraints:\n",
    "1. There is perfect information about the game: all players know all information about the state of the game and there is no hidden information. This property separates games like Nim, Tic-Tac-Toe and chess from games like Minesweeper, where the states of each square is hidden from the player.\n",
    "2. The game is deterministic: all actions are determined by players and there are no random actions (eg. dice rolling, drawing cards, flipping a coin).\n",
    "3. Only two players are involved.\n",
    "\n",
    "A finite combinatorial game is a game that will always end, ie, it has a finite sequence of moves. In order to play Nim, each player must make a move x, y in the game, where x represents the pile number and y represents the number of items removed from the pile. y must be a minimum of 1, meaning that the player must make a minimum of 1 removal. In addition, all of the removals in a single turn must be from the same pile.\n",
    "\n",
    "When played with a normal play convention, Nim ends when a player is unable to make a move and loses a game as a result. Conversely, the winner is determined by the last player to remove an item from the pile. (The misere play convention refers to the reverse, where the outcomes are flipped, but this remains out of the scope of this project.)\n",
    "\n",
    "The purpose of this project is to use reinforcement learning to elucidate the winning position of this finite combinatorial game. The winning position of this game referes to the optimal playing strategy of the game.\n",
    "\n",
    "Nim is chosen for this project because it is one of the best known examples of an impartial game - referring to the fact that both players have exactly the same moves, and the ONLY differentiating factor is that one player goes before another. Because the winning strategies of Nim have been studied extensively, this provides a baseline to gain greater understanding and mastery of the skills required before moving on to more complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bfab1",
   "metadata": {},
   "source": [
    "## AI: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ee5ec",
   "metadata": {},
   "source": [
    "The main focus of this project is to acquire the skills required to construct an artificial intelligence agent with the capabilities of using reinforcement learning to solve a problem in a dynamic environment.\n",
    "\n",
    "Reinforcement learning is the training of AI models to make decisions in a labyrinthe and dynamic game-like environment. By modelling the environment as a game, the AI model explores the environment using test-iteration and experiments with a decision. If the decision is deemed expedient, the AI would be given a \"reward\". This concept is highly analogous to that of operant conditioning in psychology, which is a method of learning that employs rewards and punishments for a particular action. If the action is deemed advantageous, it will be reinforced with rewards, causing the agent to form an association between the action and the reward. As a result, the agent will exhibit a higher proclivity to perform that action.\n",
    "\n",
    "In reinforcement learning, the AI is given no strategies or rules on how to learn the game; only a list of potential rewards for every action. By using a search algorithm, the AI is then able to perform the game, \"level up\", and maximize its rewards, going from a complete bumbling amateur to an expert with world-class, superhuman skills, as evidenced by the AI DeepBlue which beat world champion Garry Kasparov in a game of chess.\n",
    "\n",
    "To achieve the goals of this project, Q-learning will be utilized. The workings of the Q-learning algorithm will be further expounded upon under the \"Q-learning\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05983b6f",
   "metadata": {},
   "source": [
    "### Setting up the configuration of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543817fa",
   "metadata": {},
   "source": [
    "The following code has been acquired from Harvard University's Brian Yu and David Malan and deals with establishing the environment of the game in order for the AI agent to practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Nim():\n",
    "\n",
    "    def __init__(self, initial=[1, 3, 5, 7]):\n",
    "        \"\"\"\n",
    "        Initialize game board.\n",
    "        Each game board has\n",
    "            - `piles`: a list of how many elements remain in each pile\n",
    "            - `player`: 0 or 1 to indicate which player's turn\n",
    "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
    "        \"\"\"\n",
    "        self.piles = initial.copy()\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "\n",
    "    @classmethod\n",
    "    def available_actions(cls, piles):\n",
    "        \"\"\"\n",
    "        Nim.available_actions(piles) takes a `piles` list as input\n",
    "        and returns all of the available actions `(i, j)` in that state.\n",
    "\n",
    "        Action `(i, j)` represents the action of removing `j` items\n",
    "        from pile `i` (where piles are 0-indexed).\n",
    "        \"\"\"\n",
    "        actions = set()\n",
    "        for i, pile in enumerate(piles):\n",
    "            for j in range(1, pile + 1):\n",
    "                actions.add((i, j))\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        \"\"\"\n",
    "        Nim.other_player(player) returns the player that is not\n",
    "        `player`. Assumes `player` is either 0 or 1.\n",
    "        \"\"\"\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"\n",
    "        Switch the current player to the other player.\n",
    "        \"\"\"\n",
    "        self.player = Nim.other_player(self.player)\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Make the move `action` for the current player.\n",
    "        `action` must be a tuple `(i, j)`.\n",
    "        \"\"\"\n",
    "        pile, count = action\n",
    "\n",
    "        # Check for errors\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Game already won\")\n",
    "        elif pile < 0 or pile >= len(self.piles):\n",
    "            raise Exception(\"Invalid pile\")\n",
    "        elif count < 1 or count > self.piles[pile]:\n",
    "            raise Exception(\"Invalid number of objects\")\n",
    "\n",
    "        # Update pile\n",
    "        self.piles[pile] -= count\n",
    "        self.switch_player()\n",
    "\n",
    "        # Check for a winner\n",
    "        if all(pile == 0 for pile in self.piles):\n",
    "            self.winner = self.player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713f5c7",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6b5c0",
   "metadata": {},
   "source": [
    "Q learning is defined as a model-free reinforcement learning algorithm with the skill of handling dynamic environments with stochastic transitions without requiring any modifications. For any finite Markov Decision Process (FMDP), Q-learning finds the actions that maximizes the expected reward of the current state, as well as all successive states.\n",
    "\n",
    "The \"Q\" in Q-learning stands for \"Quality\" learning. The quality of each action is operationalized by its q-value; whose formula is explained under the \"update_q_value\" function.\n",
    "\n",
    "For any state s and action a, the function \"Update Q value\" performs the following value-iteration update calculation based on the Bellman equation:\n",
    "\n",
    "Q[(s,a)] <-- Q[(s,a)] + alpha (rewards - Q[(s,a)]),\n",
    "\n",
    "where alpha is the learning rate of the problem.\n",
    "\n",
    "The choose_action function gives the AI two possibilities: explore and exploit.\n",
    "\n",
    "The algorithm is trained using exploration (epsilon = True), where it makes a random choice between an optimal action (the action with the highest Q-value) and a random action (any legal action) based on a weighted probability of epsilon. Exploration allows the algorithm to discover and internalize new possibilities of acting in the environment. Through exploration, the initial Q-value may be lower relative to a more immediately aggressive algorithm; but its long-run Q value is higher, making it a more optimal algorithm.\n",
    "\n",
    "After 10000 rounds of training and building up a database of optimal moves, the algorithm is then launched using the exploit function (epsilon = False), which causes it to perform actions that maximize the Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1162f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NimAI():\n",
    "\n",
    "    def __init__(self, alpha=0.5, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize AI with an empty Q-learning dictionary,\n",
    "        an alpha (learning) rate, and an epsilon rate.\n",
    "\n",
    "        The Q-learning dictionary maps `(state, action)`\n",
    "        pairs to a Q-value (a number).\n",
    "         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n",
    "         - `action` is a tuple `(i, j)` for an action\n",
    "        \"\"\"\n",
    "        self.q = dict()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, old_state, action, new_state, reward):\n",
    "        \"\"\"\n",
    "        Update Q-learning model, given an old state, an action taken\n",
    "        in that state, a new resulting state, and the reward received\n",
    "        from taking that action.\n",
    "        \"\"\"\n",
    "        old = self.get_q_value(old_state, action)\n",
    "        best_future = self.best_future_reward(new_state)\n",
    "        self.update_q_value(old_state, action, old, reward, best_future)\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the Q-value for the state `state` and the action `action`.\n",
    "        If no Q-value exists yet in `self.q`, return 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        #state: list, eg. [1, 1, 3, 5] means that \n",
    "        #pile 0 has 1 object, pile 1 has 1 object,\n",
    "        #pile 2 has 3 objects and pile 3 has 5 objects\n",
    "        \n",
    "        #action: i, j, taking j objects from pile i\n",
    "        \n",
    "        if (tuple(state), action) in self.q.keys():\n",
    "            return self.q[tuple(state), action]\n",
    "        return 0\n",
    "\n",
    "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
    "        \n",
    "        self.q[tuple(state), action] = old_q + self.alpha * (reward + future_rewards - old_q)\n",
    "\n",
    "    def best_future_reward(self, state):\n",
    "            \n",
    "        possible_actions = Nim.available_actions(state)\n",
    "        \n",
    "        #return 0 if no available actions\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0\n",
    "        \n",
    "        max_q = -np.inf\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            q_value = self.get_q_value(state, action)\n",
    "                \n",
    "            if q_value > max_q:\n",
    "                max_q = q_value\n",
    "            \n",
    "        return max_q\n",
    "\n",
    "    def choose_action(self, state, epsilon=True):\n",
    "    \n",
    "        #obtain all possible actions in the state        \n",
    "        avail = Nim.available_actions(state)\n",
    "        \n",
    "        if len(avail) == 0:\n",
    "            print(\"No available actions in this state\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        #list of optimal actions\n",
    "        optimal_actions = []\n",
    "        \n",
    "        for a in avail:\n",
    "            if self.get_q_value(state, a) == self.best_future_reward(state):\n",
    "                optimal_actions.append(a)\n",
    "                \n",
    "        print(\"list of optimal actions\", optimal_actions)\n",
    "        \n",
    "        #if greedy algorithm and optimal actions are present:\n",
    "        if not epsilon and len(optimal_actions) >= 1:\n",
    "            act = random.choice(optimal_actions)\n",
    "            return act\n",
    "        \n",
    "        else:\n",
    "            random_action = random.choice(list(avail))\n",
    "            optimal_action = random.choice(optimal_actions)\n",
    "            act = random.choices([optimal_action, random_action], weights = \\\n",
    "                                  [1- self.epsilon, self.epsilon], k = 1)\n",
    "            return act[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee2156",
   "metadata": {},
   "source": [
    "### Training and Playing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7890db",
   "metadata": {},
   "source": [
    "After training the AI for 10000 games, the human should never be able to win against the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7df619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n):\n",
    "    \"\"\"\n",
    "    Train an AI by playing `n` games against itself.\n",
    "    \"\"\"\n",
    "\n",
    "    player = NimAI()\n",
    "\n",
    "    # Play n games\n",
    "    for i in range(n):\n",
    "        print(f\"Playing training game {i + 1}\")\n",
    "        game = Nim()\n",
    "\n",
    "        # Keep track of last move made by either player\n",
    "        last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None}\n",
    "        }\n",
    "\n",
    "        # Game loop\n",
    "        while True:\n",
    "\n",
    "            # Keep track of current state and action\n",
    "            state = game.piles.copy()\n",
    "            action = player.choose_action(game.piles)\n",
    "            print(\"action chosen by AI...\", action)\n",
    "\n",
    "            # Keep track of last state and action\n",
    "            last[game.player][\"state\"] = state\n",
    "            last[game.player][\"action\"] = action\n",
    "\n",
    "            # Make move\n",
    "            game.move(action)\n",
    "            new_state = game.piles.copy()\n",
    "\n",
    "            # When game is over, update Q values with rewards\n",
    "            if game.winner is not None:\n",
    "                player.update(state, action, new_state, -1)\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    1\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # If game is continuing, no rewards yet\n",
    "            elif last[game.player][\"state\"] is not None:\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    print(\"Done training\")\n",
    "\n",
    "    # Return the trained AI\n",
    "    return player\n",
    "\n",
    "\n",
    "def play(ai, human_player=None):\n",
    "    \"\"\"\n",
    "    Play human game against the AI.\n",
    "    `human_player` can be set to 0 or 1 to specify whether\n",
    "    human player moves first or second.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no player order set, choose human's order randomly\n",
    "    if human_player is None:\n",
    "        human_player = random.randint(0, 1)\n",
    "\n",
    "    # Create new game\n",
    "    game = Nim()\n",
    "\n",
    "    # Game loop\n",
    "    while True:\n",
    "\n",
    "        # Print contents of piles\n",
    "        print()\n",
    "        print(\"Piles:\")\n",
    "        for i, pile in enumerate(game.piles):\n",
    "            print(f\"Pile {i}: {pile}\")\n",
    "        print()\n",
    "\n",
    "        # Compute available actions\n",
    "        available_actions = Nim.available_actions(game.piles)\n",
    "        print(\"available actions...\", available_actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Let human make a move\n",
    "        if game.player == human_player:\n",
    "            print(\"Your Turn\")\n",
    "            while True:\n",
    "                pile = int(input(\"Choose Pile: \"))\n",
    "                count = int(input(\"Choose Count: \"))\n",
    "                if (pile, count) in available_actions:\n",
    "                    break\n",
    "                print(\"Invalid move, try again.\")\n",
    "\n",
    "        # Have AI make a move\n",
    "        else:\n",
    "            print(\"AI's Turn\")\n",
    "            \n",
    "            #check what output does ai.choose_action give\n",
    "            #ensure that it can be elicited in a tuple form\n",
    "            \n",
    "            pile, count = ai.choose_action(game.piles, epsilon=False)\n",
    "            print(f\"AI chose to take {count} from pile {pile}.\")\n",
    "\n",
    "        # Make move\n",
    "        game.move((pile, count))\n",
    "\n",
    "        # Check for winner\n",
    "        if game.winner is not None:\n",
    "            print()\n",
    "            print(\"GAME OVER\")\n",
    "            winner = \"Human\" if game.winner == human_player else \"AI\"\n",
    "            print(f\"Winner is {winner}\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f5f04",
   "metadata": {},
   "source": [
    "### Influence of Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354c90b",
   "metadata": {},
   "source": [
    "Alpha refers to the learning rate of the algorithm - the extent by which newly attained information overrides past knowledge. A value of 0 causes the AI to learn nothing - purely utilizing past knowledge. Although the optimal alpha for a purely deterministic game such as Nim would be 1, the agreed upon conventions for alpha is 0.1.\n",
    "\n",
    "Epsilon refers to the degree which the algorithm considers future states. With an epsilon of zero, the algorithm is \"short-sighted\", meaning that it merely searches to maximize the best Q-value 1 iteration later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74d3ca",
   "metadata": {},
   "source": [
    "## Conclusion & Project Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3ee28",
   "metadata": {},
   "source": [
    "This concludes my initial attempt at constructing a reinforcement learning algorithm. A more in depth analysis of the Bellman equation and Markov Decision Processes would allow the transfer of this algorithm, or other similar reinforcement learning algorithms, into novel situations.\n",
    "\n",
    "In addition, it would be interesting to explore how encoding the optimal solution for the Nim game performs relative to this reinforcement learning algorithm, where the algorithm is given zero knowledge about the best strategies required to solve this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
